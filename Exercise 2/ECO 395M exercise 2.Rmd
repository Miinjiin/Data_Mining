---
title: "ECO 395M exercise 2"
author: "Minjin Kang, Paul Park"
date: "2023-02-18"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggplot2)
library(rsample)  
library(caret)
library(modelr)
library(parallel)
library(foreach)
library(tidyverse)
library(mosaic)
library(dplyr)
library(ROCR)

```

## 1) Saratoga house prices

** Between linear model and K-nearest-neighbor regression model, which one is better at achieving lower
out-of-sample mean-squared error? **

```{r problem 1, message=FALSE, echo=FALSE, warning=FALSE}
# read the data
data(SaratogaHouses)

# split into training and testing set
saratoga_split = initial_split(SaratogaHouses, prop = 0.8)
saratoga_train = training(saratoga_split)
saratoga_test = testing(saratoga_split)

#1) Best linear model

# medium model we did in linear model class
lm_medium = lm(price ~ . - pctCollege - sewer - waterfront - landValue - newConstruction, data=saratoga_train)
rmse(lm_medium, saratoga_test)
#coef(lm_medium) %>% round(0)

# medium model we did in AIC class
#lm_medium = lm(price ~ lotSize + age + livingArea + pctCollege + bedrooms + fireplaces + bathrooms + rooms + heating + fuel + centralAir, data=saratoga_train)

# add meaningful interaction variable 
lm_answer = lm(price ~ . +  (livingArea:lotSize + livingArea:fuel + livingArea:centralAir + bedrooms:bathrooms + heating:fuel) - pctCollege - sewer - waterfront - landValue - newConstruction, data=saratoga_train)
rmse(lm_answer, saratoga_test)

# add not meaningful interaction variable
lm_notanswer= lm(price ~ . +  (age:livingArea + centralAir:lotSize) - pctCollege - sewer - waterfront - landValue - newConstruction, data=saratoga_train)
rmse(lm_notanswer, saratoga_test)

# medium rmse: 61107.2 , answer rmse: 59942.05 , notanswer rmse: 60834.71

#2) Best KNN regression model

knn_saratoga_split = initial_split(SaratogaHouses, prop = 0.8)
knn_saratoga_train = training(knn_saratoga_split)
knn_saratoga_test = testing(knn_saratoga_split)

# standarize before applying KNN
Xtrain=model.matrix(~ . - pctCollege - sewer - waterfront - landValue - newConstruction - 1, data=knn_saratoga_train)
Xtest=model.matrix(~ . - pctCollege - sewer - waterfront - landValue - newConstruction - 1, data=knn_saratoga_test)

# training and testing set responses
ytrain=knn_saratoga_train$price
ytest=knn_saratoga_test$price

# now rescale
scale_train = apply(Xtrain, 2, sd)
Xtilde_train = scale(Xtrain, scale=scale_train)
Xtilde_test = scale(Xtest, scale=scale_train)

# fix the range of k
k_grid=2:150

colnames(Xtilde_train)
colnames(Xtilde_test)

#finding_smallest_k = foreach(i=2:150, .combine='c') %do% {
#knn_k=knnreg(price~lotSize+age+livingArea+bedrooms+fireplaces+bathrooms+rooms+heatingho#tair+heatinghotwater/steam+heatingelectric+fuelelectric+fueloil+centralAir, #data=Xtilde_train, k=i)
#  rmse(knn_k, Xtilde_test)
#}

# KNN with the optimal value K and getting RMSE
knn =knnreg(price ~ lotSize+age+livingArea+bedrooms+fireplaces+bathrooms+rooms+heating+fuel+centralAir, data=Xtilde_train, k=k_grid[which.min(finding_smallest_k)])

rmse(knn,Xtilde_test)


```

## 2) Classification and retrospective sampling 

```{r problem 2.1, message=FALSE, echo=FALSE, warning=FALSE}
credit=read.csv('Data/german_credit.csv')

default=credit %>%
  group_by(history) %>%
  summarize(average_default=mean(Default)) %>%
  as.data.frame()

default %>%
  ggplot(aes(x=history, y=average_default, fill=history)) +
  geom_col()+
  labs(x="Credit history",
       y="Default probability",
       title="Predictng default by credit history ",
       subtitle="(german_credit)") + 
  theme_bw() +
  theme(plot.title = element_text(face="bold"))

```

**Figure 1:** Bar plot showing average default probability by credit history.


```{r problem 2.2, message=FALSE, echo=FALSE, warning=FALSE}
# split training and testing data
credit_split=initial_split(credit, prop= 0.8)
credit_train=training(credit_split)
credit_test=testing(credit_split)

# logistic regression
logistic=glm(Default ~ duration+amount+installment+age+history+purpose+foreign, data=credit_train, family=binomial)

coef_df<- coef(logistic)%>% as.data.frame %>% round(2)

knitr::kable(coef_df)


credit_history = credit %>%
  group_by(history) %>%
  summarise(count = n ()) %>%
  as.data.frame()

summarise(credit_history)

```


The bar plot shows that a borrower with a 'good' credit rating history has the highest probability of defaulting on a loan, followed by 'poor,' and 'terrible.' Similar result is again found in the logistic regression model, which shows that 'historyterrible' and 'historybad' are more negatively correlated with 'Default,' than the intercept (historygood). Both results indicate that a borrower with a good credit history is more likely to default on his/her loan, which is **highly counter-intuitive.**

This data set is not appropriate for predicting "high" vs. "low" probably of default when screening prospective borrowers. Sampling defaulted loans and matching similar kinds of loans will inevitably lead to a biased prediction because the majority of samples collected are borrowers with poor and terrible credit rating **as shown above.** Rather, the bank should use a random selection when collecting samples to better predict the probability of defaults.



## 3) Children and hotel reservations

### Model building

```{r problem 3.1, message=FALSE, echo=FALSE}

dev=read.csv('Data/hotels_dev.csv')

# split into training and testing set
dev_split = initial_split(dev, prop = 0.8)
dev_train = training(dev_split)
dev_test = testing(dev_split)

# baseline1: using only four features
b1 = dev %>%
  select(children, market_segment, adults, customer_type, is_repeated_guest)
# building linear model using four features
b1_lm = lm(children ~ market_segment + adults + customer_type +is_repeated_guest, data=dev_train, family=binomial)
# examine the fitted coefficients
coef(b1_lm) %>% round(3)


# baseline2: using all predictor except the arrival date
b2 = dev[, colnames(dev)[colnames(dev)!='arrival_date']]
# building linear model
b2_lm=lm(children ~ . - arrival_date, data=dev_train, family=binomial)
# examine the fitted coefficients
coef(b1_lm) %>% round(3)


# build the best linear model, similar method that we used for number 1, and we'll add some interaction variable which seems highly correlated

# let's see columns names first and pick ones that seems related 
colnames(dev)
# see if the type is integer or character
str(dev)
# result: some columns are int, some columns are chr

# Add interaction variables and made sample best linear model
b_best_lm=lm(children ~ . - arrival_date + adults:stays_in_weekend_nights + adults:total_of_special_requests + adults:average_daily_rate + average_daily_rate:total_of_special_requests, data=dev_train)



# calculating the confusion matrix which tabulates predicted class versus actual class , tresholding predicted probabilities at 0.5

phat_b1=predict(b1_lm,dev_test)
yhat_b1=ifelse(phat_b1 > 0.5, 1, 0)
confusion_b1=table(y=dev_test$children, yhat=yhat_b1)

phat_b2=predict(b2_lm,dev_test)
yhat_b2=ifelse(phat_b2 > 0.5, 1, 0)
confusion_b2=table(y=dev_test$children, yhat=yhat_b2)

phat_b_best=predict(b_best_lm,dev_test)
yhat_b_best=ifelse(phat_b_best > 0.5, 1, 0)
confusion_b_best=table(y=dev_test$children, yhat=yhat_b_best)

knitr::kable(confusion_b1)
knitr::kable(confusion_b2)
knitr::kable(confusion_b_best)



```

### Model validation : STEP 1 ### 

```{r problem 3.2.1, message=FALSE, echo=FALSE}
val=read.csv('Data/hotels_val.csv')

# split into training and testing set
val_split = initial_split(val, prop = 0.8)
val_train = training(val_split)
val_test = testing(val_split)


phat_val_best=predict(b_best_lm, val_test, type = "response")

roc = foreach(i=1:90, .combine='c') %do% {
  yhat_val_best=ifelse(phat_val_best >= (i/100), 1, 0)
  confusion_val_best=table(y=val_test$children, yhat=yhat_val_best)
  TN=confusion_val_best[1,1]
  FP=confusion_val_best[1,2]
  FN=confusion_val_best[2,1]
  TP=confusion_val_best[2,2]
  TPR=TP/(FN+TP)
  FPR=FP/(TN+FP)
  FPR
  TPR
}

roc_df_=data.frame(roc)

roc_df

ggplot(roc_df)+
  geom_line(aes(x=roc_fpr, y=roc_tpr)) +
  labs(title="ROC curve",
       x="FPR",
       y="TPR") +
       #subtitle="(Year 1896 Olympic Medalist)") +
  theme_classic() +
  theme(plot.title = element_text(face="bold"))



```
```{r problem 3.2.2, message=FALSE, echo=FALSE}

phat_val_best=predict(b_best_lm, val_test, type = "response")
val_test$pred_child <- phat_val_best
pred <- prediction(val_test$pred_child, val_test$children)
perf <- performance(pred,"tpr","fpr")
plot(perf)

```
```{r problem 3.2.3, message=FALSE, echo=FALSE, error=FALSE, warning=FALSE}

# create 20 folds
K=20

# create specific fold IDs for each row
val=val %>%
  mutate(fold_id=rep(1:K, length=nrow(val)) %>% sample())

# split into training and testing set
val_split = initial_split(val, prop = 0.8)
val_train = training(val_split)
val_test = testing(val_split)

# making vector set for expected number of bookings with children
expected_child=c()
# making vector set for actual number of bookings with children
actual_child=c()

# main code for k fold
fold_cv = foreach(i=1:K, .combine='c') %do% {
  fold=val%>%filter(fold_id==i)
  model_cv=glm(children ~ . - arrival_date+adults:stays_in_weekend_nights + adults:total_of_special_requests + adults:average_daily_rate + average_daily_rate:total_of_special_requests, data=val)
  phat_cv=predict(model_cv, fold, type="response")
  sum_phat=sum(phat_cv)
  expected_child=c(expected_child,sum_phat)
  sum_actual=sum(fold$children)
  actual_child=c(actual_child,sum_actual)
}

# setting range of fold 
fold=c(1:20)

# making dataframe having three columns, fold id / expected number of children / actual number of children
fold_cv_df=data.frame(fold,expected_child,actual_child)

knitr::kable(fold_cv_df)

```


```{r problem 3.2.4, message=FALSE, echo=FALSE, fig.width = 25}



df2 <- melt(fold_cv_df, id.vars='fold')
head(df2)
ggplot(df2, aes(x=fold, y=value, fill=variable),ylim=c(0, 40)) +
    geom_bar(stat='identity', position=position_dodge(), width=0.8) + 
  scale_x_continuous(breaks=seq(1,20,1)) +
  scale_y_continuous(expand=expansion(mult=c(0,0.05)))




```









