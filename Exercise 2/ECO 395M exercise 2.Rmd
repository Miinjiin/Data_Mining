---
title: "ECO 395M exercise 2"
author: "Minjin Kang, Paul Park"
date: "2023-02-18"
output: md_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggplot2)
library(rsample)  
library(caret)
library(modelr)
library(parallel)
library(foreach)
library(tidyverse)
library(mosaic)
library(dplyr)
library(ROCR)
library(reshape2)

```

## 1) Saratoga house prices

We used `SaratogaHouses` data to build the optimal pricing model.
We first separated `SaratogaHouses` data into a training and a testing set, then used the testing set to assess the model's out-of-sample performance. 

For our best linear model, we added some interaction variables like `livingArea:lotSize` `livingArea:fuel` `livingArea:centralAir` `bedrooms:bathrooms` `heating:fuel`.
We compared the RMSEs of the "medium" model and our model to evaluate the performance.

```{r problem 1, message=FALSE, echo=FALSE, warning=FALSE}
# read the data
data(SaratogaHouses)

# split into training and testing set
saratoga_split = initial_split(SaratogaHouses, prop = 0.8)
saratoga_train = training(saratoga_split)
saratoga_test = testing(saratoga_split)

#1) Best linear model

# medium model we did in linear model class
lm_medium = lm(price ~ . - pctCollege - sewer - waterfront - landValue - newConstruction, data=saratoga_train)
rmse_for_med = rmse(lm_medium, saratoga_test)
#coef(lm_medium) %>% round(0)

# medium model we did in AIC class
#lm_medium = lm(price ~ lotSize + age + livingArea + pctCollege + bedrooms + fireplaces + bathrooms + rooms + heating + fuel + centralAir, data=saratoga_train)

# add meaningful interaction variable 
lm_answer = lm(price ~ . +  (livingArea:lotSize + livingArea:fuel + livingArea:centralAir + bedrooms:bathrooms + heating:fuel) - pctCollege - sewer - waterfront - landValue - newConstruction, data=saratoga_train)
rmse_for_ours=rmse(lm_answer, saratoga_test)

# add not meaningful interaction variable
lm_notanswer= lm(price ~ . +  (age:livingArea + centralAir:lotSize) - pctCollege - sewer - waterfront - landValue - newConstruction, data=saratoga_train)
rmse_for_not_meaningful_interaction=rmse(lm_notanswer, saratoga_test)
```

```{r problem 1.1, message=FALSE, echo=FALSE, warning=FALSE}
medium_rmse<-rmse(lm_medium,saratoga_test)
answer_rmse<-rmse(lm_answer,saratoga_test)

medium_rmse
answer_rmse

```

Looking at above table, our model does a slightly better job at achieving lower out-of-sample mean-squared error.


```{r problem 1.2, message=FALSE, echo=FALSE, warning=FALSE}

#2) Best KNN regression model

knn_saratoga_split = initial_split(SaratogaHouses, prop = 0.8)
knn_saratoga_train = training(knn_saratoga_split)
knn_saratoga_test = testing(knn_saratoga_split)

# remove columns which has chr value and standarize before applying KNN
Xtrain=model.matrix(~ . - pctCollege - sewer - waterfront - landValue - newConstruction - 1, data=knn_saratoga_train)
Xtest=model.matrix(~ . - pctCollege - sewer - waterfront - landValue - newConstruction - 1, data=knn_saratoga_test)

# training and testing set responses
ytrain=knn_saratoga_train$price
ytest=knn_saratoga_test$price

# now rescale
scale_train = apply(Xtrain, 2, sd)
Xtilde_train = scale(Xtrain, scale=scale_train)
Xtilde_test = scale(Xtest, scale=scale_train)


#Xtilde_test=data.frame(Xtilde_test)%>%
#  mutate(price=c(ytest))
#Xtilde_train=data.frame(Xtilde_train)%>%
#  mutate(price=c(ytrain))

Xtrain_add <- subset(knn_saratoga_train, select = c("pctCollege","sewer","waterfront","landValue","newConstruction"))
Xtest_add <- subset(knn_saratoga_test, select = c("pctCollege","sewer","waterfront","landValue","newConstruction"))

# add chr value columns again
Xtrain_full = data.frame(Xtilde_train,Xtrain_add) %>% as.data.frame()
Xtest_full = data.frame(Xtilde_test,Xtest_add) %>% as.data.frame()


#rmse_out_saratoga=foreach(i=2:150, .combine='c') %do% {
#  knn_model_saratoga= knnreg(price ~ . - pctCollege - sewer - waterfront - landValue - newConstruction, #data=Xtrain_full, k=i)
#  modelr::rmse(knn_model_saratoga,Xtest_full)
#}

#rmse_out_saratoga

k_grid=2:150


finding_smallest_k = foreach(i=k_grid, .combine='c') %do% {
  knn_k=knnreg(price ~ ., data=Xtrain_full, k=i)
  rmse_k=rmse(knn_k,Xtest_full)
}


#finding_smallest_k = foreach(i=k_grid, .combine='c') %do% {
#  knn_k=knnreg(price ~ . - pctCollege - sewer - waterfront - landValue - #newConstruction, data=Xtrain_full, k=i)
#  rmse_k=rmse(knn_k,Xtest_full)
#}

#rmse == min(finding_smallest_k$rmse)
#rmse

finding_smallest_k = finding_smallest_k %>% as.data.frame()

best_k=which(finding_smallest_k == min(finding_smallest_k))

best_k

#knn_best_k=knnreg(price ~ . - pctCollege - sewer - waterfront - landValue - newConstruction, data=Xtrain_full, k=2)
#rmse(knn_best_k,Xtest_full)

# KNN with the optimal value K and getting RMSE
#knn =knnreg(price ~ lotSize+age+livingArea+bedrooms+fireplaces+bathrooms+rooms+heating+fuel+centralAir, data=Xtilde_train, k=k_grid[which.min(finding_smallest_k)])

#rmse(knn,Xtilde_test)


```

To build the best KNN regression model, we first found the optimal K, **`r best_k`**, that produces the lowest RMSE value. This value was used in the "horse race" between the two model classes.


```{r problem 1.2.1, message=FALSE, echo=FALSE, warning=FALSE}

compare_two_model= do(150)*{
  compare_two_split = initial_split(SaratogaHouses, prop = 0.8)
  compare_two_train = training(compare_two_split)
  compare_two_test = testing(compare_two_split)
  
  Xtrain=model.matrix(~ . - pctCollege - sewer - waterfront - landValue - newConstruction - 1, data=compare_two_train)
  Xtest=model.matrix(~ . - pctCollege - sewer - waterfront - landValue - newConstruction - 1, data=compare_two_test)
  
  ytrain=compare_two_train$price
  ytest=compare_two_test$price
  
  scale_train = apply(Xtrain, 2, sd)
  Xtilde_train = scale(Xtrain, scale=scale_train)
  Xtilde_test = scale(Xtest, scale=scale_train)
  
  Xtrain_add <- subset(compare_two_train, select = c("pctCollege","sewer","waterfront","landValue","newConstruction"))
  Xtest_add <- subset(compare_two_test, select = c("pctCollege","sewer","waterfront","landValue","newConstruction"))
  
  Xtrain_full = data.frame(Xtilde_train,Xtrain_add) %>% as.data.frame()
  Xtest_full = data.frame(Xtilde_test,Xtest_add) %>% as.data.frame()
  
  knn_compare=knnreg(price ~ ., data=compare_two_train, k=best_k)
  rmse(knn_compare,compare_two_test)

  
  lm_compare = lm(price ~ . +  (livingArea:lotSize + livingArea:fuel + livingArea:centralAir + bedrooms:bathrooms + heating:fuel) - pctCollege - sewer - waterfront - landValue - newConstruction, data=compare_two_train)
  rmse(lm_compare,compare_two_test)
  
  c(rmse(knn_compare,compare_two_test),rmse(lm_compare,compare_two_test))

}

knitr:: kable(head(compare_two_model), col.names = c("RMSE_KNN","RMSE_LM"))
```

```{r problem 1.2.2, message=FALSE, echo=FALSE, warning=FALSE}
knitr:: kable(colMeans(compare_two_model))
```

For the comparison, we ran 150 random train/test splits. First table shows the first six results, which shows that the RMSE of our model repeatedly beats that of the KNN model.

Looking at the second table, we can observe that our linear model has the lower **average** out-of-sample RMSE compared to that of the KNN model.

**Report on findings:**  On top of the "medium" model, we added what we considered to be meaningful interaction variables like `livingArea:lotSize` `livingArea:fuel` `livingArea:centralAir` `bedrooms:bathrooms` and `heating:fuel`. We assumed that the relationships between the two variables in each interaction were important in their effect on the housing price. For example, the effect of the living area size on the house price would be different depending on whether or not the house has a central air. Number of bedrooms would affect the house price differently based on how many bathrooms there are. A house with four bedrooms with four bathrooms would certainly be priced differently compared to one with four bedrooms and one bathroom! Our inclusion of such interactions in the model proved to be a good decision as it out-performed both the "medium" model and the KNN model, achieving the lowest out-of-sample mean-squared error.


## 2) Classification and retrospective sampling 


Likelihood that one will default based on their credit history(Good,Poor,Terrible).

```{r problem 2.1, message=FALSE, echo=FALSE, warning=FALSE}
# import data set
credit=read.csv('Data/german_credit.csv')

# group by history and calculate mean default value for each
default=credit %>%
  group_by(history) %>%
  summarize(average_default=mean(Default)) %>%
  as.data.frame()

# make bar plot of default probability by credit history
default %>%
  ggplot(aes(x=history, y=average_default, fill=history)) +
  geom_col()+
  labs(x="Credit history",
       y="Default probability",
       title="Predictng default by credit history ",
       subtitle="(german_credit)") + 
  theme_bw() +
  theme(plot.title = element_text(face="bold"))

```

**Plot 1:** Bar plot showing average default probability by credit history.


```{r problem 2.2, message=FALSE, echo=FALSE, warning=FALSE}

# split training and testing data
credit_split=initial_split(credit, prop= 0.8)
credit_train=training(credit_split)
credit_test=testing(credit_split)

# logistic regression
logistic=glm(Default ~ duration+amount+installment+age+history+purpose+foreign, data=credit_train, family=binomial)

coef_df<- coef(logistic)%>% as.data.frame %>% round(2)

# count values by history group
credit_history = credit %>%
  group_by(history) %>%
  summarise(count = n ()) %>%
  as.data.frame()

knitr::kables(
  list(
  knitr:: kable(coef_df, valign='t'),
  knitr:: kable(credit_history,col.names = c("history", "count"))
  )
)
```


The bar plot shows that a borrower with a 'good' credit rating history has the highest probability of defaulting on a loan, followed by 'poor,' and 'terrible.' Similar result is again found in the logistic regression model, which shows that 'historyterrible' and 'historybad' are more negatively correlated with 'Default,' than the intercept (historygood). Both results indicate that a borrower with a good credit history is more likely to default on his/her loan, which is **highly counter-intuitive,** which made us suspect something was not right.

This data set is not appropriate for predicting "high" vs. "low" probably of default when screening prospective borrowers. Sampling defaulted loans and matching similar kinds of loans will inevitably lead to a biased prediction because the majority of samples collected are borrowers with poor and terrible credit rating as shown in the table above. Rather, the bank should use a random selection (and/or a much larger sample size) when collecting samples to better predict the probability of defaults.






## 3) Children and hotel reservations

### Model building

Here, we will build a predictive model for whether a hotel booking will have children on it. We start by splitting train/test data with `hotels_dev.csv`, and making baseline 1 and baseline 2 models as instructed.

For the best linear model, we added what we considered to be meaningful interactions, such as `adults:stays_in_weekend_nights`, `adults:total_of_special_requests`, `adults:average_daily_rate`, and `average_daily_rate:total_of_special_request`.

```{r problem 3.1, message=FALSE, echo=FALSE, warning=FALSE}

dev=read.csv('Data/hotels_dev.csv')

# split into training and testing set
dev_split = initial_split(dev, prop = 0.8)
dev_train = training(dev_split)
dev_test = testing(dev_split)

# baseline1: using only four features

b1 = dev %>%
  select(children, market_segment, adults, customer_type, is_repeated_guest)
# building linear model using four features
b1_lm = lm(children ~ market_segment + adults + customer_type +is_repeated_guest, data=dev_train, family=binomial)
# examine the fitted coefficients
coef1= coef(b1_lm) %>% round(3)


# baseline2: using all predictor except the arrival date

b2 = dev[, colnames(dev)[colnames(dev)!='arrival_date']]
# building linear model
b2_lm=lm(children ~ . - arrival_date, data=dev_train, family=binomial)
# examine the fitted coefficients
coef2= coef(b1_lm) %>% round(3)


# build the best linear model, similar method that we used for number 1, and we'll add some interaction variable which seems highly correlated

# Add interaction variables and made sample best linear model
b_best_lm=lm(children ~ . - arrival_date + adults:stays_in_weekend_nights + adults:total_of_special_requests + adults:average_daily_rate + average_daily_rate:total_of_special_requests, data=dev_train)



# calculating the confusion matrix which tabulates predicted class versus actual class , tresholding predicted probabilities at 0.5

# Confusion matrix for baseline 1
phat_b1=predict(b1_lm,dev_test)
yhat_b1=ifelse(phat_b1 > 0.5, 1, 0)
confusion_b1=table(y=dev_test$children, yhat=yhat_b1)

# Confusion matrix for baseline 2
phat_b2=predict(b2_lm,dev_test)
yhat_b2=ifelse(phat_b2 > 0.5, 1, 0)
confusion_b2=table(y=dev_test$children, yhat=yhat_b2)

# Confusion matrix for best linear model we made
phat_b_best=predict(b_best_lm,dev_test)
yhat_b_best=ifelse(phat_b_best > 0.5, 1, 0)
confusion_b_best=table(y=dev_test$children, yhat=yhat_b_best)


```

Under is the predictions on the testing set.
We will use this to measure out-of-sample performance.




```{r problem 3.1.1, message=FALSE, echo=FALSE}

knitr::kables(
  list(
  knitr:: kable(confusion_b1, valign='t'),
  knitr:: kable(round(sum(diag(confusion_b1))/sum(confusion_b1) * 100, 2))
  )
)

```

**Table set 1:** Left is a Confusion matrix for baseline 1 model that used only four features.
On its right is the out-of-sample accuracy.

Looking at above confusion matrix, one may ask why is there a missing column? That is because the baseline 1 model never predicted children!






```{r problem 3.1.2, message=FALSE, echo=FALSE}

knitr::kables(
  list(
  knitr:: kable(confusion_b2, valign='t'),
  knitr:: kable(round(sum(diag(confusion_b2))/sum(confusion_b2) * 100, 2))
  )
)
```

**Table set 2:** Left table is Confusion matrix for baseline 2 model that used all possible predictors except for arrival_date.
On its right is the out-of-sample accuracy.

One can see that baseline 2 model performs better than base line 1 model.








```{r problem 3.1.3, message=FALSE, echo=FALSE}

knitr::kables(
  list(
  knitr:: kable(confusion_b_best, valign='t'),
  knitr:: kable(round(sum(diag(confusion_b_best))/sum(confusion_b_best) * 100, 2))
  )
)
```

**Table set 3:** Left table is Confusion matrix for the best linear model we made with additional interaction variables.
On its right is the out-of-sample accuracy.

Our model has a slightly higher accuracy than baseline 2 model.






### Model validation : STEP 1 ### 

Here, we validate our best model by testing it on an entirely fresh data, `hotels_val.csv`

```{r problem 3.2.1, message=FALSE, echo=FALSE}
val=read.csv('Data/hotels_val.csv')

#b_best_lm=lm(children ~ . - arrival_date + adults:stays_in_weekend_nights + adults:total_of_special_requests + adults:average_daily_rate + average_daily_rate:total_of_special_requests, data=dev_train)


phat_val_best=predict(b_best_lm, val, type = "response")


#roc= foreach(i=1:90, .combine='c') %do% {
#  yhat_val_best=ifelse(phat_val_best >= (i/100), 1, 0)
#  confusion_val_best=table(y=val$children, yhat=yhat_val_best)
#  TN=confusion_val_best[1,1]
#  FP=confusion_val_best[1,2]
#  FN=confusion_val_best[2,1]
#  TP=confusion_val_best[2,2]
#  FPR=FP/(TN+FP)
#  TPR=TP/(FN+TP)
#  TPR
#  FPR
#}
#roc

roc_tpr= foreach(i=1:90, .combine='c') %do% {
  yhat_val_best=ifelse(phat_val_best >= (i/100), 1, 0)
  confusion_val_best=table(y=val$children, yhat=yhat_val_best)
  TN=confusion_val_best[1,1]
  FP=confusion_val_best[1,2]
  FN=confusion_val_best[2,1]
  TP=confusion_val_best[2,2]
  TPR=TP/(FN+TP)
  TPR
}

roc_fpr= foreach(i=1:90, .combine='c') %do% {
  yhat_val_best=ifelse(phat_val_best >= (i/100), 1, 0)
  confusion_val_best=table(y=val$children, yhat=yhat_val_best)
  TN=confusion_val_best[1,1]
  FP=confusion_val_best[1,2]
  FN=confusion_val_best[2,1]
  TP=confusion_val_best[2,2]
  FPR=FP/(TN+FP)
  FPR
  }

roc_df=data.frame(roc_fpr,roc_tpr)

knitr::kable(head(roc_df),col.names=c("ROC_FPR","ROC_TPR"))

```

To produce an ROC curve plot, we first made a dataframe consisting of False Positive Rate values and True Positive Rate values for each threshold t. Above are the first six results using head(df) command.


```{r problem 3.2.1.2, message=FALSE, echo=FALSE}
ggplot(roc_df)+
  geom_line(aes(x=roc_fpr, y=roc_tpr), color="blue") +
  labs(title="ROC curve for best linear model we built",
       x="FPR(False Positive Rate)",
       y="TPR(True Postive Rate)") +
  theme_classic() +
  theme(plot.title = element_text(face="bold"))

```

**Plot 2:** ROC curve (FPR(t) vs TPR(t)) for our best model, using new dataset `hotels_val.csv`


While working through the problem, we came across a cool package called `ROCR`. Just for the kicks, we tried generating the ROC curve again using this package, which generated a similar curve as above.

```{r problem 3.2.2, message=FALSE, echo=FALSE}

phat_val_best=predict(b_best_lm, val, type = "response")
val$pred_child <- phat_val_best
pred <- prediction(val$pred_child, val$children)
perf <- performance(pred,"tpr","fpr")
plot(perf, main="ROC curve generated from ROCR package", col='blue')

```


**Plot 2:**  ROC curve (using `ROCR` packcage) showing True Positive Rate against the False Positive Rate for different threshold t. This took less time and fewer lines of codes. Simple and fast!








### Model validation : STEP 2 ### 

Let's create 20 folds of `hotels_val.csv`.
For each fold set, we have 250 bookings showing expected number of children and actual number of children.


```{r problem 3.2.3, message=FALSE, echo=FALSE, error=FALSE, warning=FALSE}

# create 20 folds
K=20

# create specific fold IDs for each row
val=val %>%
  mutate(fold_id=rep(1:K, length=nrow(val)) %>% sample())

# split into training and testing set
val_split = initial_split(val, prop = 0.8)
val_train = training(val_split)
val_test = testing(val_split)

# making vector set for expected number of bookings with children
expected_child=c()
# making vector set for actual number of bookings with children
actual_child=c()

# main code for k fold
fold_cv = foreach(i=1:K, .combine='c') %do% {
  fold=val%>%filter(fold_id==i)
  model_cv=glm(children ~ . - arrival_date+adults:stays_in_weekend_nights + adults:total_of_special_requests + adults:average_daily_rate + average_daily_rate:total_of_special_requests, data=val)
  phat_cv=predict(model_cv, fold, type="response")
  sum_phat=sum(phat_cv)
  expected_child=c(expected_child,sum_phat)
  sum_actual=sum(fold$children)
  actual_child=c(actual_child,sum_actual)
}

# setting range of fold 
fold=c(1:20)

# making dataframe having three columns, fold id / expected number of children / actual number of children
fold_cv_df=data.frame(fold,expected_child,actual_child)

knitr::kable(fold_cv_df)

```

**Table 4:** 20 folds each with expected number of children (left) and actual number of children (right) for comparison.



```{r problem 3.2.4, message=FALSE, echo=FALSE, fig.width = 17}

df2 <- melt(fold_cv_df, id.vars='fold')
ggplot(df2, aes(x=fold, y=value, fill=variable),ylim=c(0, 40)) +
  geom_bar(stat='identity', position=position_dodge(), width=0.8) + 
    labs(title="Expected vs Actual num of children for 20 folds",
       x="K Fold",
       y="Number of bookings having children") +
  scale_x_continuous(breaks=seq(1,20,1)) +
  scale_y_continuous(expand=expansion(mult=c(0,0.05)))

```
**Plot 3:** Grouped bar plot showing 20 fold sets comparing expected number of children and actual number of children (Red=expected, Green=actual).

As one can clearly see from **Table 4** and **Plot 3**, our model does a pretty accurate job at predicting the total number of bookings with children in a group of 250 bookings overall.








