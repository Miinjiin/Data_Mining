---
title: "ECO 395M exercise 2"
author: "Minjin Kang, Paul Park"
date: "2023-02-18"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggplot2)
library(rsample)  
library(caret)
library(modelr)
library(parallel)
library(foreach)
library(tidyverse)
library(mosaic)
library(dplyr)


```

## 1) Saratoga house prices

** Between linear model and K-nearest-neighbor regression model, which one is better at achieving lower
out-of-sample mean-squared error? **

```{r problem 1, message=FALSE, echo=FALSE, warning=FALSE}
# read the data
data(SaratogaHouses)

# split into training and testing set
saratoga_split = initial_split(SaratogaHouses, prop = 0.8)
saratoga_train = training(saratoga_split)
saratoga_test = testing(saratoga_split)

#1) Best linear model

# medium model we did in linear model class
lm_medium = lm(price ~ . - pctCollege - sewer - waterfront - landValue - newConstruction, data=saratoga_train)
rmse(lm_medium, saratoga_test)
#coef(lm_medium) %>% round(0)

# medium model we did in AIC class
#lm_medium = lm(price ~ lotSize + age + livingArea + pctCollege + bedrooms + fireplaces + bathrooms + rooms + heating + fuel + centralAir, data=saratoga_train)

# add meaningful interaction variable 
lm_answer = lm(price ~ . +  (livingArea:lotSize + livingArea:fuel + livingArea:centralAir + bedrooms:bathrooms + heating:fuel) - pctCollege - sewer - waterfront - landValue - newConstruction, data=saratoga_train)
rmse(lm_answer, saratoga_test)

# add not meaningful interaction variable
lm_notanswer= lm(price ~ . +  (age:livingArea + centralAir:lotSize) - pctCollege - sewer - waterfront - landValue - newConstruction, data=saratoga_train)
rmse(lm_notanswer, saratoga_test)

# medium rmse: 61107.2 , answer rmse: 59942.05 , notanswer rmse: 60834.71

#2) Best KNN regression model

knn_saratoga_split = initial_split(SaratogaHouses, prop = 0.8)
knn_saratoga_train = training(knn_saratoga_split)
knn_saratoga_test = testing(knn_saratoga_split)

# standarize before applying KNN
Xtrain=model.matrix(~ . - pctCollege - sewer - waterfront - landValue - newConstruction - 1, data=knn_saratoga_train)
Xtest=model.matrix(~ . - pctCollege - sewer - waterfront - landValue - newConstruction - 1, data=knn_saratoga_test)

# training and testing set responses
ytrain=knn_saratoga_train$price
ytest=knn_saratoga_test$price

# now rescale
scale_train = apply(Xtrain, 2, sd)
Xtilde_train = scale(Xtrain, scale=scale_train)
Xtilde_test = scale(Xtest, scale=scale_train)

# fix the range of k
k_grid=2:150

colnames(Xtilde_train)
colnames(Xtilde_test)

#finding_smallest_k = foreach(i=2:150, .combine='c') %do% {
#knn_k=knnreg(price~lotSize+age+livingArea+bedrooms+fireplaces+bathrooms+rooms+heatingho#tair+heatinghotwater/steam+heatingelectric+fuelelectric+fueloil+centralAir, #data=Xtilde_train, k=i)
#  rmse(knn_k, Xtilde_test)
#}

# KNN with the optimal value K and getting RMSE
knn =knnreg(price ~ lotSize+age+livingArea+bedrooms+fireplaces+bathrooms+rooms+heating+fuel+centralAir, data=Xtilde_train, k=k_grid[which.min(finding_smallest_k)])

rmse(knn,Xtilde_test)


```

## 2) Classification and retrospective sampling 

```{r problem 2.1, message=FALSE, echo=FALSE, warning=FALSE}
credit=read.csv('Data/german_credit.csv')

default=credit %>%
  group_by(history) %>%
  summarize(average_default=mean(Default)) %>%
  as.data.frame()

default %>%
  ggplot(aes(x=history, y=average_default, fill=history)) +
  geom_col()+
  labs(x="Credit history",
       y="Default probability",
       title="Predictng default by credit history ",
       subtitle="(german_credit)") + 
  theme_bw() +
  theme(plot.title = element_text(face="bold"))

```

```{r problem 2.2, message=FALSE, echo=FALSE, warning=FALSE}
# split training and testing data
credit_split=initial_split(credit, prop= 0.8)
credit_train=training(credit_split)
credit_test=testing(credit_split)

# logistic regression
logistic=glm(Default ~ duration+amount+installment+age+history+purpose+foreign, data=credit_train, family=binomial)

coef_df<- coef(logistic)%>% as.data.frame %>% round(2)

knitr::kable(coef_df)

```





## 3) Children and hotel reservations

### Model building

```{r problem 3, message=FALSE, echo=FALSE}

dev=read.csv('Data/hotels_dev.csv')
val=read.csv('Data/hotels_val.csv')

# split into training and testing set
dev_split = initial_split(dev, prop = 0.8)
dev_train = training(dev_split)
dev_test = testing(dev_split)

# baseline1: using only four features
b1 = dev %>%
  select(market_segment, adults, customer_type, is_repeated_guest)
# building linear model
b1_lm = lm(children ~ market_segment + adults + customer_type +is_repeated_guest, data=dev_train, family=binomial)


# baseline2: using all predictor except the arrival date
b2 = dev[, colnames(dev)[colnames(dev)!='arrival_date']]
# building linear model
b2_lm=lm(children ~ . - arrival_date, data=dev_train, family=binomial)


# we should build the best linear model, similar method we used for number 1 we'll add some interaction variable which seems highly correlated

# let's see columns names first and pick ones that seems related 
colnames(dev)
# see if the type is integer or character
str(dev)
# result: some columns are int, some columns are chr

# I picked adults & stays in the weekend nights
b_best_lm=lm(children ~ . - arrival_date + adults:stays_in_weekend_nights, data=dev_train, family=binomial)

# we should add bunch of interaction variables and had to check which one has the lowest rmse


```





