knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggplot2)
library(rsample)
library(caret)
library(modelr)
library(parallel)
library(foreach)
library(tidyverse)
library(mosaic)
library(dplyr)
library(ROCR)
library(reshape2)
data(SaratogaHouses)
# split into training and testing set
saratoga_split = initial_split(SaratogaHouses, prop = 0.8)
saratoga_train = training(saratoga_split)
saratoga_test = testing(saratoga_split)
#1) Best linear model
# medium model we did in linear model class
lm_medium = lm(price ~ . - pctCollege - sewer - waterfront - landValue - newConstruction, data=saratoga_train)
rmse_for_med = rmse(lm_medium, saratoga_test)
#coef(lm_medium) %>% round(0)
# medium model we did in AIC class
#lm_medium = lm(price ~ lotSize + age + livingArea + pctCollege + bedrooms + fireplaces + bathrooms + rooms + heating + fuel + centralAir, data=saratoga_train)
# add meaningful interaction variable
lm_answer = lm(price ~ . +  (livingArea:lotSize + livingArea:fuel + livingArea:centralAir + bedrooms:bathrooms + heating:fuel) - pctCollege - sewer - waterfront - landValue - newConstruction, data=saratoga_train)
rmse_for_ours=rmse(lm_answer, saratoga_test)
# add not meaningful interaction variable
lm_notanswer= lm(price ~ . +  (age:livingArea + centralAir:lotSize) - pctCollege - sewer - waterfront - landValue - newConstruction, data=saratoga_train)
rmse_for_not_meaningful_interaction=rmse(lm_notanswer, saratoga_test)
```
lm_notanswer= lm(price ~ . +  (age:livingArea + centralAir:lotSize) - pctCollege - sewer - waterfront - landValue - newConstruction, data=saratoga_train)
rmse_for_not_meaningful_interaction=rmse(lm_notanswer, saratoga_test)
```
data(SaratogaHouses)
# split into training and testing set
saratoga_split = initial_split(SaratogaHouses, prop = 0.8)
saratoga_train = training(saratoga_split)
saratoga_test = testing(saratoga_split)
lm_medium = lm(price ~ . - pctCollege - sewer - waterfront - landValue - newConstruction, data=saratoga_train)
rmse_for_med = rmse(lm_medium, saratoga_test)
lm_answer = lm(price ~ . +  (livingArea:lotSize + livingArea:fuel + livingArea:centralAir + bedrooms:bathrooms + heating:fuel) - pctCollege - sewer - waterfront - landValue - newConstruction, data=saratoga_train)
rmse_for_ours=rmse(lm_answer, saratoga_test)
medium_rmse<-rmse(lm_medium,saratoga_test)
answer_rmse<-rmse(lm_answer,saratoga_test)
medium_rmse
answer_rmse
nn_saratoga_split = initial_split(SaratogaHouses, prop = 0.8)
knn_saratoga_train = training(knn_saratoga_split)
Xtrain_add <- subset(knn_saratoga_train, select = c("pctCollege","sewer","waterfront","landValue","newConstruction"))
Xtrain_add <- subset(knn_saratoga_train, select = c("pctCollege","sewer","waterfront","landValue","newConstruction"))
Xtrain_add <- subset(knn_saratoga_train, select = c("pctCollege","sewer","waterfront","landValue","newConstruction"))
knn_saratoga_split = initial_split(SaratogaHouses, prop = 0.8)
knn_saratoga_train = training(knn_saratoga_split)
knn_saratoga_test = testing(knn_saratoga_split)
Xtrain=model.matrix(~ . - pctCollege - sewer - waterfront - landValue - newConstruction - 1, data=knn_saratoga_train)
Xtest=model.matrix(~ . - pctCollege - sewer - waterfront - landValue - newConstruction - 1, data=knn_saratoga_test)
ytrain=knn_saratoga_train$price
ytest=knn_saratoga_test$price
scale_train = apply(Xtrain, 2, sd)
Xtilde_train = scale(Xtrain, scale=scale_train)
Xtilde_test = scale(Xtest, scale=scale_train)
Xtrain_add <- subset(knn_saratoga_train, select = c("pctCollege","sewer","waterfront","landValue","newConstruction"))
Xtest_add <- subset(knn_saratoga_test, select = c("pctCollege","sewer","waterfront","landValue","newConstruction"))
Xtrain_full = data.frame(Xtilde_train,Xtrain_add) %>% as.data.frame()
Xtest_full = data.frame(Xtilde_test,Xtest_add) %>% as.data.frame()
k_grid=2:150
finding_smallest_k = foreach(i=k_grid, .combine='c') %do% {
knn_k=knnreg(price ~ ., data=Xtrain_full, k=i)
rmse_k=rmse(knn_k,Xtest_full)
}
finding_smallest_k = foreach(i=k_grid, .combine='c') %do% {
knn_k=knnreg(price ~ ., data=Xtrain_full, k=i)
rmse_k=rmse(knn_k,Xtest_full)
}
finding_smallest_k = finding_smallest_k %>% as.data.frame()
best_k=which(finding_smallest_k == min(finding_smallest_k))
best_k
compare_two_model= do(150)*{
compare_two_split = initial_split(SaratogaHouses, prop = 0.8)
compare_two_train = training(compare_two_split)
compare_two_test = testing(compare_two_split)
Xtrain=model.matrix(~ . - pctCollege - sewer - waterfront - landValue - newConstruction - 1, data=compare_two_train)
Xtest=model.matrix(~ . - pctCollege - sewer - waterfront - landValue - newConstruction - 1, data=compare_two_test)
ytrain=compare_two_train$price
ytest=compare_two_test$price
scale_train = apply(Xtrain, 2, sd)
Xtilde_train = scale(Xtrain, scale=scale_train)
Xtilde_test = scale(Xtest, scale=scale_train)
Xtrain_add <- subset(compare_two_train, select = c("pctCollege","sewer","waterfront","landValue","newConstruction"))
Xtest_add <- subset(compare_two_test, select = c("pctCollege","sewer","waterfront","landValue","newConstruction"))
Xtrain_full = data.frame(Xtilde_train,Xtrain_add) %>% as.data.frame()
Xtest_full = data.frame(Xtilde_test,Xtest_add) %>% as.data.frame()
knn_compare=knnreg(price ~ ., data=compare_two_train, k=best_k)
rmse(knn_compare,compare_two_test)
lm_compare = lm(price ~ . +  (livingArea:lotSize + livingArea:fuel + livingArea:centralAir + bedrooms:bathrooms + heating:fuel) - pctCollege - sewer - waterfront - landValue - newConstruction, data=compare_two_train)
rmse(lm_compare,compare_two_test)
c(rmse(knn_compare,compare_two_test),rmse(lm_compare,compare_two_test))
}
knitr:: kable(head(compare_two_model), col.names = c("RMSE_KNN","RMSE_LM"))
```
knitr:: kable(colMeans(compare_two_model), row.names = c("RMSE_KNN","RMSE_LM"))
knitr:: kable(colMeans(compare_two_model))
credit=read.csv('Data/german_credit.csv')
# group by history and calculate mean default value for each
default=credit %>%
group_by(history) %>%
summarize(average_default=mean(Default)) %>%
as.data.frame()
# make bar plot of default probability by credit history
default %>%
ggplot(aes(x=history, y=average_default, fill=history)) +
geom_col()+
labs(x="Credit history",
y="Default probability",
title="Predictng default by credit history ",
subtitle="(german_credit)") +
theme_bw() +
theme(plot.title = element_text(face="bold"))
credit=read.csv('Data/german_credit.csv')
# group by history and calculate mean default value for each
default=credit %>%
group_by(history) %>%
summarize(average_default=mean(Default)) %>%
as.data.frame()
# make bar plot of default probability by credit history
default %>%
ggplot(aes(x=history, y=average_default, fill=history)) +
geom_col()+
labs(x="Credit history",
y="Default probability",
title="Predictng default by credit history ",
subtitle="(german_credit)") +
theme_bw() +
theme(plot.title = element_text(face="bold"))
credit_split=initial_split(credit, prop= 0.8)
credit_train=training(credit_split)
credit_test=testing(credit_split)
# logistic regression
logistic=glm(Default ~ duration+amount+installment+age+history+purpose+foreign, data=credit_train, family=binomial)
coef_df<- coef(logistic)%>% as.data.frame %>% round(2)
# count values by history group
credit_history = credit %>%
group_by(history) %>%
summarise(count = n ()) %>%
as.data.frame()
knitr::kables(
list(
knitr:: kable(coef_df, valign='t'),
knitr:: kable(credit_history,col.names = c("history", "count"))
)
)
```
credit_split=initial_split(credit, prop= 0.8)
credit_train=training(credit_split)
credit_test=testing(credit_split)
logistic=glm(Default ~ duration+amount+installment+age+history+purpose+foreign, data=credit_train, family=binomial)
coef_df<- coef(logistic)%>% as.data.frame %>% round(2)
credit_history = credit %>%
group_by(history) %>%
summarise(count = n ()) %>%
as.data.frame()
knitr::kables(
list(
knitr:: kable(coef_df, valign='t'),
knitr:: kable(credit_history,col.names = c("history", "count"))
)
)
```
# count values by history group
credit_history = credit %>%
group_by(history) %>%
summarise(count = n ()) %>%
as.data.frame()
knitr::kables(
list(
knitr:: kable(coef_df, valign='t'),
knitr:: kable(credit_history,col.names = c("history", "count"))
)
)
dev=read.csv('Data/hotels_dev.csv')
# split into training and testing set
dev_split = initial_split(dev, prop = 0.8)
dev_train = training(dev_split)
dev_test = testing(dev_split)
b1 = dev %>%
select(children, market_segment, adults, customer_type, is_repeated_guest)
# building linear model using four features
b1_lm = lm(children ~ market_segment + adults + customer_type +is_repeated_guest, data=dev_train, family=binomial)
# examine the fitted coefficients
coef1= coef(b1_lm) %>% round(3)
b2 = dev[, colnames(dev)[colnames(dev)!='arrival_date']]
# building linear model
b2_lm=lm(children ~ . - arrival_date, data=dev_train, family=binomial)
# examine the fitted coefficients
coef2= coef(b1_lm) %>% round(3)
b_best_lm=lm(children ~ . - arrival_date + adults:stays_in_weekend_nights + adults:total_of_special_requests + adults:average_daily_rate + average_daily_rate:total_of_special_requests, data=dev_train)
phat_b1=predict(b1_lm,dev_test)
yhat_b1=ifelse(phat_b1 > 0.5, 1, 0)
confusion_b1=table(y=dev_test$children, yhat=yhat_b1)
# Confusion matrix for baseline 2
phat_b2=predict(b2_lm,dev_test)
yhat_b2=ifelse(phat_b2 > 0.5, 1, 0)
confusion_b2=table(y=dev_test$children, yhat=yhat_b2)
# Confusion matrix for best linear model we made
phat_b_best=predict(b_best_lm,dev_test)
yhat_b_best=ifelse(phat_b_best > 0.5, 1, 0)
confusion_b_best=table(y=dev_test$children, yhat=yhat_b_best)
```
phat_b1=predict(b1_lm,dev_test)
yhat_b1=ifelse(phat_b1 > 0.5, 1, 0)
confusion_b1=table(y=dev_test$children, yhat=yhat_b1)
phat_b2=predict(b2_lm,dev_test)
yhat_b2=ifelse(phat_b2 > 0.5, 1, 0)
confusion_b2=table(y=dev_test$children, yhat=yhat_b2)
phat_b_best=predict(b_best_lm,dev_test)
yhat_b_best=ifelse(phat_b_best > 0.5, 1, 0)
confusion_b_best=table(y=dev_test$children, yhat=yhat_b_best)
nitr::kables(
list(
knitr:: kable(confusion_b1, valign='t'),
knitr:: kable(round(sum(diag(confusion_b1))/sum(confusion_b1) * 100, 2))
)
)
knitr::kables(
list(
knitr:: kable(confusion_b1, valign='t'),
knitr:: kable(round(sum(diag(confusion_b1))/sum(confusion_b1) * 100, 2))
)
)
knitr::kables(
list(
knitr:: kable(confusion_b2, valign='t'),
knitr:: kable(round(sum(diag(confusion_b2))/sum(confusion_b2) * 100, 2))
)
)
knitr::kables(
list(
knitr:: kable(confusion_b_best, valign='t'),
knitr:: kable(round(sum(diag(confusion_b_best))/sum(confusion_b_best) * 100, 2))
)
)
```
knitr::kables(
list(
knitr:: kable(confusion_b_best, valign='t'),
knitr:: kable(round(sum(diag(confusion_b_best))/sum(confusion_b_best) * 100, 2))
)
)
val=read.csv('Data/hotels_val.csv')
#b_best_lm=lm(children ~ . - arrival_date + adults:stays_in_weekend_nights + adults:total_of_special_requests + adults:average_daily_rate + average_daily_rate:total_of_special_requests, data=dev_train)
phat_val_best=predict(b_best_lm, val, type = "response")
roc_tpr= foreach(i=1:90, .combine='c') %do% {
yhat_val_best=ifelse(phat_val_best >= (i/100), 1, 0)
confusion_val_best=table(y=val$children, yhat=yhat_val_best)
TN=confusion_val_best[1,1]
FP=confusion_val_best[1,2]
FN=confusion_val_best[2,1]
TP=confusion_val_best[2,2]
TPR=TP/(FN+TP)
TPR
}
roc_fpr= foreach(i=1:90, .combine='c') %do% {
yhat_val_best=ifelse(phat_val_best >= (i/100), 1, 0)
confusion_val_best=table(y=val$children, yhat=yhat_val_best)
TN=confusion_val_best[1,1]
FP=confusion_val_best[1,2]
FN=confusion_val_best[2,1]
TP=confusion_val_best[2,2]
FPR=FP/(TN+FP)
FPR
}
roc_df=data.frame(roc_fpr,roc_tpr)
knitr::kable(head(roc_df),col.names=c("ROC_FPR","ROC_TPR"))
ggplot(roc_df)+
geom_line(aes(x=roc_fpr, y=roc_tpr), color="blue") +
labs(title="ROC curve for best linear model we built",
x="FPR(False Positive Rate)",
y="TPR(True Postive Rate)") +
theme_classic() +
theme(plot.title = element_text(face="bold"))
phat_val_best=predict(b_best_lm, val, type = "response")
val$pred_child <- phat_val_best
pred <- prediction(val$pred_child, val$children)
perf <- performance(pred,"tpr","fpr")
plot(perf, main="ROC curve generated from ROCR package", col='blue')
```
phat_val_best=predict(b_best_lm, val, type = "response")
val$pred_child <- phat_val_best
pred <- prediction(val$pred_child, val$children)
perf <- performance(pred,"tpr","fpr")
plot(perf, main="ROC curve generated from ROCR package", col='blue')
K=20
# create specific fold IDs for each row
val=val %>%
mutate(fold_id=rep(1:K, length=nrow(val)) %>% sample())
# split into training and testing set
val_split = initial_split(val, prop = 0.8)
val_train = training(val_split)
val_test = testing(val_split)
# making vector set for expected number of bookings with children
expected_child=c()
# making vector set for actual number of bookings with children
actual_child=c()
# main code for k fold
fold_cv = foreach(i=1:K, .combine='c') %do% {
fold=val%>%filter(fold_id==i)
model_cv=glm(children ~ . - arrival_date+adults:stays_in_weekend_nights + adults:total_of_special_requests + adults:average_daily_rate + average_daily_rate:total_of_special_requests, data=val)
phat_cv=predict(model_cv, fold, type="response")
sum_phat=sum(phat_cv)
expected_child=c(expected_child,sum_phat)
sum_actual=sum(fold$children)
actual_child=c(actual_child,sum_actual)
}
# setting range of fold
fold=c(1:20)
# making dataframe having three columns, fold id / expected number of children / actual number of children
fold_cv_df=data.frame(fold,expected_child,actual_child)
knitr::kable(fold_cv_df)
df2 <- melt(fold_cv_df, id.vars='fold')
ggplot(df2, aes(x=fold, y=value, fill=variable),ylim=c(0, 40)) +
geom_bar(stat='identity', position=position_dodge(), width=0.8) +
labs(title="Expected vs Actual num of children for 20 folds",
x="K Fold",
y="Number of bookings having children") +
scale_x_continuous(breaks=seq(1,20,1)) +
scale_y_continuous(expand=expansion(mult=c(0,0.05)))
