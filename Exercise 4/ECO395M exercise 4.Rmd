---
title: "ECO 395M exercise 4"
author: "Phillip An, Paul Park, Min Jin Kang"
date: "2023-04-07"
output: md_document
always_allow_html: true

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(foreach)
library(mosaic)
library(corrplot)
library(dplyr)
library(knitr)
library(kableExtra)
library(reshape2)
library(ggplot2)
library(rsample)  
library(caret)
library(modelr)
library(arules)
library(arulesViz)
library(igraph)
library(DiagrammeR)
```


## 1) Clustering and PCA


## K-means Clustering
```{r 1.1.1, message=FALSE, echo=FALSE, warning=FALSE}

wine = read.csv('Data/wine.csv')

summary(wine)
View(wine)

# Center and scale the data
X = wine[,-(12:13)]
X = scale(X, center=TRUE, scale=TRUE)


# Extract the centers and scales from the rescaled data (which are named attributes)
mu = attr(X,"scaled:center")
sigma = attr(X,"scaled:scale")


# Using kmenas
clust_wine = kmeans(X, 2, nstart = 10)

# Alcohol and pH
ggplot(wine) + 
  geom_point(aes(alcohol, pH, color = factor(clust_wine$cluster), shape=factor(clust_wine$cluster)))+
  labs(x ="Alcohol", y ="pH ", title = "Cluster Membership")

```


```{r 1.1.2, message=FALSE, echo=FALSE, warning=FALSE}

ggplot(wine) + 
  geom_point(aes(alcohol, pH, color = color , shape=factor(clust_wine$cluster)))+  guides(shape =guide_legend(title="Cluster"))+
  labs(x ="alcohol", y ="pH ", title = " Is white wine separated from red wine?")

```


```{r 1.1.3, message=FALSE, echo=FALSE, warning=FALSE}
### residual sugar and density
ggplot(wine) + 
  geom_point(aes(volatile.acidity, residual.sugar, color = factor(clust_wine$cluster), shape=factor(clust_wine$cluster)))+
  labs(x ="volatile acidity ", y ="residual sugar ", title = "Cluster Membership")

```


```{r 1.1.4, message=FALSE, echo=FALSE, warning=FALSE}
ggplot(wine) + 
  geom_point(aes(volatile.acidity, residual.sugar, color = color , shape=factor(clust_wine$cluster)))+ guides(shape =guide_legend(title="Cluster"))+
  labs(x ="volatile acidity", y ="residual sugar", title = "Is white wine separated from red wine?")

```


```{r 1.1.5, message=FALSE, echo=FALSE, warning=FALSE}

ggplot(wine) + 
  geom_point(aes(volatile.acidity, total.sulfur.dioxide, color = quality, shape=factor(clust_wine$cluster)))+ 
  guides(shape =guide_legend(title="Cluster"))+
  labs( title = "Wine Quality?")

```


## PCA

```{r 1.2.1, message=FALSE, echo=FALSE, warning=FALSE}
######## PCA
#Setup
pca_wine = prcomp(X, rank=6, scale=TRUE)
loadings = pca_wine$rotation
scores = pca_wine$x

#linear combinations
kable(head(loadings))

summary(pca_wine)
```


```{r 1.2.2, message=FALSE, echo=FALSE, warning=FALSE}

qplot(scores[,1], scores[,3], color=wine$color, xlab='Component 1', ylab='Component 3') + scale_color_manual(values=c("red", "grey"))

```


```{r 1.2.3, message=FALSE, echo=FALSE, warning=FALSE}

qplot(scores[,1], scores[,3], color=wine$quality, xlab='Component 1', ylab='Component 3')

```



## 2) Market segmentation

Method to be used is `K-means clustering`. With this method, we will be able to detect interesting market segments that seem to be exceptional within NutrientH20's social-media audience. 

Read the `social_marketing.csv` and let's take a look for the 36 different categories
\newline
```{r 2.1.1, message=FALSE, echo=FALSE, warning=FALSE}
# read the data
data=read.csv('Data/social_marketing.csv')

# let's explore data
c=colnames(data)
col_list=matrix(c[2:37], nrow=6, ncol=6 ,byrow=TRUE)
col_table=as.table(col_list)
knitr::kable(col_table, col.names= NULL, row.names = FALSE, caption="36 different categories of interest") %>% kable_minimal()


```


With K-means clustering, we can cluster these categories, for example, "physical wellness" can be one cluster containing `personal_fitness`, `health_nutrition`, `outdoors`.

\newline

As mentioned in the question, we can check there are categories like `spam`, `adult`, `uncategorized`, and I will remove `spam` and `adult` to clean our dataset. 


```{r 2.1.1.2, message=FALSE, echo=FALSE, warning=FALSE}

# drop first column, which user has labeled as random 9-digit code
X=data[,-1]
# drop spam and adult column which slip through the data
X=X[,-(35:36)]
# center and scale
X=scale(X, center=TRUE, scale=TRUE)
# Extract the centers and scales from the rescaled data (which are named attributes)
mu = attr(X,"scaled:center")
sigma = attr(X,"scaled:scale")


```

After cleaning/centering/scaling the data, I will start with correlation plot for k-means clustering, as correlation plot can visualize which categories in the dataset are strongly correlated each other, and also can identify which categories have similar scales. 

```{r 2.1.2, message=FALSE, echo=FALSE, warning=FALSE}

corrplot::corrplot(cor(X), method='color', addCoef.col=0.3, number.cex = 0.2, tl.cex=0.3)

```

As there's so many variables, I will sort highest correlations. 
```{r 2.1.3, message=FALSE, echo=FALSE, warning=FALSE}

corr_highest <- function(){
  corr = cor(X)
  # drop duplicates, and correlation 1
  corr[lower.tri(corr,diag=TRUE)] <- NA 
  corr[corr == 1] <- NA 
  # removing na values and making dataframe
  corr = as.data.frame(as.table(corr))
  corr = na.omit(corr) 
  # select significant values (Freq more than 0.6)
  corr = subset(corr, abs(Freq) > 0.6) 
  # sort highest corr
  corr = corr[order(-abs(corr$Freq)),] 
  mtx_corr = reshape2::acast(corr, Var1~Var2, value.var="Freq")
  corrplot(mtx_corr, is.corr=FALSE, na.label=" ", tl.cex=0.9, col = colorRampPalette(c("#FDE2E2", "#FAA2A2", "#F55D5D", "#D92727", "#A51414"))(100))
  knitr::kable(corr,row.names = FALSE, caption="Highest correlation among categories") %>% kable_minimal()
}
corr_highest()
```

As you can see, `health_nutrition` and `personal_fitness` has the highest correlation and this is exactly what I thought would be in the same cluster - "physical wellness". 

\newline

We've finished previewing for k-means clustering with correlation plot, and we'll start analyzing with k-means clustering.

\newline
\newline
\newline
\newline

### K-means clustering

First, will start from choosing optimal K, the amount of clusters.
Below is Elbow plot. Elbow plot used to determine the optimal number of clustering. The plot displays within-cluster sum of squares(WSS) as a function of the number of clusters.

```{r 2.2.1, message=FALSE, echo=FALSE, warning=FALSE}
# use the code we learned during the class
k_grid = seq(2, 30, by=1)
SSE_grid = foreach(k = k_grid, .combine='c') %do% {
  cluster_k = kmeans(X, k, nstart=25)
  cluster_k$tot.withinss
}

# plotting elbow plot to see 
plot(k_grid, SSE_grid, main="Elbow Plot") 


```

\newline
\newline

10 seems to me to be the elbow point, so we'll use 10 for k.

\newline
\newline

We can get surface-level information about market segments for NutrientH20. 

```{r 2.2.2, message=FALSE, echo=FALSE, warning=FALSE, including=FALSE}

# Run k-means with 10 clusters and 25 starts
clu= kmeans(X, centers=10, nstart=25)

# make an empty list to store kable output
kable_output <- list()

# loop through each plot and sort by the first column in ascending order, then print the top 5 rows
for (i in 0:9) {
  plot_name <- paste0("cluster", i+1)
  plot <- data.frame(clu$center[i+1,]*sigma + mu)
  plot_sorted_top5 <- plot %>% arrange(desc(plot[1])) %>% head(5)
  names(plot_sorted_top5)[1] <- plot_name
  kable_output[[i+1]] <- plot_sorted_top5
}

# print the kable output for all 10 plots
knitr::kable(kable_output)%>% kable_minimal() %>% kable_styling(full_width = FALSE, position="left", latex_options="scale_down")

```

\newline
\newline

Above is the table for the highest five categories for each clusters in total of 10 clusters.
It appears that there are distinct groups of categories in each clusters. 
Cluster with college_uni, online_gaming, sports_playing in top 5 categories will explain NutrientH20's social-media audience is male college students whose in their early 20s. 
Cluster with news, politics, automotive, sports_fandom in top 5 categories will explain NutrientH20's social-media audience can be guys in their 30s, 40s. 
Cluster with health_nutrition, personal_fitness, outdoors in top 5 categories will explain NutrientH20's social-media audience is people interested in exercise and health. 
With this result, NutrientH20 can do marketing through gyms, product advertisements in men's magazines, or selling products in college vending machines.

\newline
\newline




## 3) Association rules for grocery purchases

###  Analysis 

`groceries.txt` file contains a total of 9,835 unique shopping baskets. We frst went through some data wrangling process before conducting Market Basket Analysis using the "arules" package. As for the thresholds, we chose support of .001, confidence of .5, and maxlen of 10. A relatively low support of .001 was chosen because we wanted to capture as many items as possible from the dataset. Confidence of .5 was chosen to sort out weak associations. Lastly, we limited the maximum number of items per item set to be 10 to account for as many possible grocery combinations as possible. Running the algorithm using the above threshold resulted in 5,668 rules, which we thought was enough for this analysis. Below are two plots showing the resulting rules; the first is plotted between support and lift, while the second is between support and confidence.


```{r problem 3, message=FALSE, echo=FALSE, warning=FALSE, include=FALSE}

#read groceries.txt file
groceries=read.table('Data/groceries.txt', sep = '\n', header = FALSE)

# using add_column()
groceries <- groceries %>%
  add_column(person = "", .before = 1)

# add user number to each row
groceries$person <- seq.int(nrow(groceries))

# split grocery items by comma and turn person into a factor

groceries = strsplit(groceries$V1, ",")
groceries$person = factor(groceries$person)

## Remove duplicates ("de-dupe")
groceries = lapply(groceries, unique)

## Cast this variable as a special arules "transactions" class.
groceriestrans = as(groceries, "transactions")

# Now run the 'apriori' algorithm
# Look at rules with support > .005 & confidence >.1 & length (# grocery items) <= 5
groceriesrules = apriori(groceriestrans, 
                     parameter=list(support=.001, confidence=.5, maxlen=10))
```

```{r problem 3-1, message=FALSE, echo=FALSE, warning=FALSE}
# plot  rules in (support, confidence) space
plot(groceriesrules, measure = c("support", "lift"), shading = "confidence")

# "two key" plot: coloring is by size (order) of item set
plot(groceriesrules, method='two-key plot')

# Choose a subset
subset = subset(groceriesrules, subset=support > .001 & confidence > .5)

rules_df <- arules::DATAFRAME(groceriesrules)

```


###  Findings 

Below is a table that shows the top ten rules with the highest confidence. Confidence shows the probability of having item(s) on the RHS given those on the LHS are purchased. You can see that out of the top ten rules, the most frequent RHS items are `whole milk` and `other vegetables`. However, the association rules here are not very interesting nor revealing. Take `{canned fish,hygiene articles} -> {whole milk}` as an example. Intuitively, buying canned fish and hygiene articles doesn't seem to have anything to do with buying whole milk. However, this this is still at the top of the list simply because whole milk gets bought the most frequently when people go grocery shopping, regardless of what other items they purchase. To see more relevant and revealing association rules, let's look at a list with the highest lift.


```{r problem 3-2, message=FALSE, echo=FALSE, warning=FALSE}

rules_df_confidence <- rules_df %>%
  arrange(desc(confidence)) %>%
  head(10) %>%
  kable(caption = 'Top 10 rules with the highest confidence')

rules_df_confidence
```


Below is a table showing the top ten rules with the highest lift. Lift is different from confidence in that it is the ratio between confidence and expected confidence. In other words, lift measures the relative strength of association between LHS and RHS. It takes care of the high frequency issue of whole milk purchase we observed above. Lift > 1 indicates that the association rule improves the chances of outcome, where as lift < 1 reveals that the model lowers the chance of the outcome. Lift = 1 does not have any effect on the outcome. The result here is much more interesting and informative.

`{popcorn,soda} -> {salty snack}` Here, it seems like people are getting ready for a movie night. People who buy popcorn and soda are likely to buy other salty snacks. Thus, the model makes sense.

`{ham,processed cheese} -> {white bread}`These are ingredients to make a quick sandwich. People who buy ham and processed cheese are likely to buy white bread to make a sandwich. Hence, the rule makes sense again.

```{r problem 3-3, message=FALSE, echo=FALSE, warning=FALSE}

rules_df_lift <- rules_df %>%
  arrange(desc(lift)) %>%
  head(10) %>%
  kable(caption = 'Top 10 rules with the highest lift')

rules_df_lift
```

The last plot is a graph-visualization representing the association rules. Each item in the LHS is connected with to the RHS item, and the arrows indicate the direction of the relationship. To avoid over-crowded plot, we have limited to rules with lift > 10.

```{r problem 3-4, message=FALSE, echo=FALSE, warning=FALSE}


# graph-based visualization
# For rules, each item in the LHS is connected with a directed edge to the item in the RHS.
#lift set higher than 10 to avoid overly-clustered plot.
groceries_graph = associations2igraph(subset(groceriesrules, lift >10), associationsAsNodes = FALSE)
igraph::write_graph(groceries_graph, file='groceries.graphml', format = "graphml")
mygraph <- read_graph('groceries.graphml', format='graphml')
plot(mygraph)

```
